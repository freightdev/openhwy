use candle_core::{Device, Tensor, Result, DType};
use std::time::Instant;

// Simulate LoRA: W' = W + BA where B is rank x d_model, A is d_model x rank
fn lora_forward(
    x: &Tensor,
    w: &Tensor,
    lora_a: &Tensor,
    lora_b: &Tensor,
    alpha: f32,
) -> Result<Tensor> {
    // Standard forward: x @ W
    let base_out = x.matmul(w)?;
    
    // LoRA path: x @ A @ B * (alpha/rank)
    let lora_out = x.matmul(lora_a)?.matmul(lora_b)?;
    let lora_out = (lora_out * alpha as f64)?;
    
    // Combine
    let out = base_out + lora_out;
    Ok(out?)
}

fn main() -> Result<()> {
    let device = Device::new_cuda(0)?;
    println!("ðŸŽ¯ LoRA/QLoRA Operations Test\n");
    
    let d_model = 4096;
    let seq_len = 512;
    let batch = 4;
    
    println!("Model config: d_model={}, seq_len={}, batch={}\n", d_model, seq_len, batch);
    
    // Test different LoRA ranks
    for rank in [8, 16, 32, 64] {
        println!("LoRA rank {}:", rank);
        
        // Base weight (frozen)
        let w = Tensor::randn(0f32, 1.0, (d_model, d_model), &device)?
            .to_dtype(DType::F16)?;
        
        // LoRA adapters (trainable)
        let lora_a = Tensor::randn(0f32, 0.01, (d_model, rank), &device)?
            .to_dtype(DType::F16)?;
        let lora_b = Tensor::randn(0f32, 0.01, (rank, d_model), &device)?
            .to_dtype(DType::F16)?;
        
        // Input
        let x = Tensor::randn(0f32, 1.0, (batch * seq_len, d_model), &device)?
            .to_dtype(DType::F16)?;
        
        // Benchmark
        device.synchronize()?;
        let start = Instant::now();
        
        for _ in 0..10 {
            let _out = lora_forward(&x, &w, &lora_a, &lora_b, 16.0)?;
        }
        
        device.synchronize()?;
        let time = start.elapsed().as_secs_f64() / 10.0;
        
        // Calculate parameters
        let base_params = d_model * d_model;
        let lora_params = d_model * rank + rank * d_model;
        let reduction = 100.0 * (1.0 - lora_params as f64 / base_params as f64);
        
        println!("   Forward pass: {:.2}ms", time * 1000.0);
        println!("   Trainable params: {} ({:.1}% of base)", 
            lora_params, 100.0 - reduction);
        println!("   Memory for weights: {} MB (vs {} MB base)\n", 
            (lora_params * 2) / 1024 / 1024,
            (base_params * 2) / 1024 / 1024);
    }
    
    println!("ðŸ“Š LoRA Efficiency:");
    println!("   7B model base: ~14GB (FP16)");
    println!("   7B model + LoRA r=16: ~14.2GB (+200MB)");
    println!("   Multiple LoRA adapters: Swap in/out as needed");
    println!("   QLoRA (4-bit base + FP16 adapters): ~4GB total âœ…");
    
    Ok(())
}
